{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training a Language Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sfeYKrAg7QM",
        "colab_type": "text"
      },
      "source": [
        "How to train a “small” model (84 M parameters = 6 layers, 768 hidden size, 12 attention heads) – that’s the same number of layers & heads as DistilBERT – on Esperanto. We’ll then fine-tune the model on a downstream task of part-of-speech tagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK7PPVm2XBgr",
        "colab_type": "text"
      },
      "source": [
        "## 1. Find a dataset\n",
        "The Esperanto portion of the dataset is only 299M, so we’ll concatenate with the Esperanto sub-corpus of the [Leipzig Corpora Collection](https://wortschatz.uni-leipzig.de/en/download), which is comprised of text from diverse sources like news, literature, and wikipedia.\n",
        "\n",
        "The final training corpus has a size of 3 GB, which is still small – for your model, you will get better results the more data you can get to pretrain on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOk4iZ9YZvec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fd722c4d-35cf-480f-bdc9-56ea2c077991"
      },
      "source": [
        "!wget -c https://s3.amazonaws.com/datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-02 13:13:26--  https://s3.amazonaws.com/datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.113.85\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.113.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 312733741 (298M) [text/plain]\n",
            "Saving to: ‘oscar.eo.txt’\n",
            "\n",
            "oscar.eo.txt        100%[===================>] 298.25M  16.5MB/s    in 20s     \n",
            "\n",
            "2020-05-02 13:13:46 (15.3 MB/s) - ‘oscar.eo.txt’ saved [312733741/312733741]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-kkz81OY6xH",
        "colab_type": "text"
      },
      "source": [
        "## 2. Train a tokenizer\n",
        "\n",
        "We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let’s arbitrarily pick its size to be 52,000.\n",
        "\n",
        "We recommend training a byte-level BPE (rather than let’s say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5duRggBRZKvP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "e254eb1a-ca8d-4ad7-dfd2-21ffecafcae3"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git \\\n",
        "    ;cd transformers \\\n",
        "    ;pip install . "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (361/361), done.\u001b[K\n",
            "remote: Compressing objects: 100% (189/189), done.\u001b[K\n",
            "remote: Total 26096 (delta 178), reused 322 (delta 163), pack-reused 25735\u001b[K\n",
            "Receiving objects: 100% (26096/26096), 15.38 MiB | 9.11 MiB/s, done.\n",
            "Resolving deltas: 100% (18216/18216), done.\n",
            "Processing /content/transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.3)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.1.86)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.0.41)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.4.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.8.0-cp36-none-any.whl size=595726 sha256=41605a9471418b226c4a7a70139ea638a7fedafc224ab7caafc8aa1a3198a585\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1ewc0__f/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.5.2\n",
            "    Uninstalling tokenizers-0.5.2:\n",
            "      Successfully uninstalled tokenizers-0.5.2\n",
            "  Found existing installation: transformers 2.8.0\n",
            "    Uninstalling transformers-2.8.0:\n",
            "      Successfully uninstalled transformers-2.8.0\n",
            "Successfully installed tokenizers-0.7.0 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMnymRDLe0hi",
        "colab_type": "code",
        "outputId": "e6178e38-e6ed-4db8-fab5-4e3391a6fc58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time \n",
        "from pathlib import Path\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 57s, sys: 2.69 s, total: 5min\n",
            "Wall time: 4min 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ei7bqpRf1LH",
        "colab_type": "text"
      },
      "source": [
        "Now let's save files to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIS-irI0f32P",
        "colab_type": "code",
        "outputId": "57754c47-4ffc-46d2-b6ab-f0077d6167c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir EsperBERTo\n",
        "tokenizer.save(\"EsperBERTo\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['EsperBERTo/vocab.json', 'EsperBERTo/merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOOfYSuQhSqT",
        "colab_type": "text"
      },
      "source": [
        "We now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency, and a `merges.txt` list of merges.\n",
        "\n",
        "```json\n",
        "{\n",
        "\t\"<s>\": 0,\n",
        "\t\"<pad>\": 1,\n",
        "\t\"</s>\": 2,\n",
        "\t\"<unk>\": 3,\n",
        "\t\"<mask>\": 4,\n",
        "\t\"!\": 5,\n",
        "\t\"\\\"\": 6,\n",
        "\t\"#\": 7,\n",
        "\t\"$\": 8,\n",
        "\t\"%\": 9,\n",
        "\t\"&\": 10,\n",
        "\t\"'\": 11,\n",
        "\t\"(\": 12,\n",
        "\t\")\": 13,\n",
        "\t# ...\n",
        "}\n",
        "\n",
        "# merges.txt\n",
        "l a\n",
        "Ġ k\n",
        "o n\n",
        "Ġ la\n",
        "t a\n",
        "Ġ e\n",
        "Ġ d\n",
        "Ġ p\n",
        "# ...\n",
        "```\n",
        "\n",
        "What is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Esperanto – `ĉ`, `ĝ`, `ĥ`, `ĵ`, `ŝ`, and `ŭ` – are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained GPT-2 tokenizer.\n",
        "\n",
        "Here’s  how you can use it in `tokenizers`, including handling the RoBERTa special tokens – of course, you’ll also be able to use it direcly from `transformers`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKVWB8WShT-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./EsperBERTo/vocab.json\",\n",
        "    \"./EsperBERTo/merges.txt\",\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO5M3vrAhcuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Ye27nchfzq",
        "colab_type": "code",
        "outputId": "edd547ec-474e-45c5-dfe5-fb579d822242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.encode(\"Mi estas Julien.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing, original_str, normalized_str])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ya5_7rhjKS",
        "colab_type": "code",
        "outputId": "a3bb3a9c-7341-4335-b61a-7703004f5ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.encode(\"Mi estas Julien.\").tokens"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'Mi', 'Ġestas', 'ĠJuli', 'en', '.', '</s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQpUC_CDhnWW",
        "colab_type": "text"
      },
      "source": [
        "## 3. Train a language model from scratch\n",
        "\n",
        "We will now train our language model using the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py) script from `transformers`. Just remember to leave `--model_name_or_path` to `None` to train from scratch vs. from an existing model or checkpoint.\n",
        "\n",
        "> We’ll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details).\n",
        "\n",
        "As the model is BERT-like, we’ll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNZZs-r6iKAV",
        "colab_type": "code",
        "outputId": "c536cf4a-aadc-4226-d0c9-21b975415905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check that PyTorch sees it\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBtUHRMliOLM",
        "colab_type": "text"
      },
      "source": [
        "Here, as we only have one text file, we don't even need to customize our `LineByLineDataset`. We'll just run the `run_language_modeling.py` script out-of-the-box."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTgWPa9Dipk2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0779e71b-3280-441d-c7a7-d2e05883efa8"
      },
      "source": [
        "# Get the example scripts.\n",
        "!wget -c https://raw.githubusercontent.com/huggingface/transformers/master/examples/run_language_modeling.py"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-02 13:31:04--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/run_language_modeling.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10500 (10K) [text/plain]\n",
            "Saving to: ‘run_language_modeling.py’\n",
            "\n",
            "\rrun_language_modeli   0%[                    ]       0  --.-KB/s               \rrun_language_modeli 100%[===================>]  10.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-05-02 13:31:05 (97.7 MB/s) - ‘run_language_modeling.py’ saved [10500/10500]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0qQzgrBi1OX",
        "colab_type": "text"
      },
      "source": [
        "### We'll define the following config for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwZXcYMujOsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "config = {\n",
        "\t\"architectures\": [\n",
        "\t\t\"RobertaForMaskedLM\"\n",
        "\t],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"roberta\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 6,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": 52000\n",
        "}\n",
        "with open(\"./EsperBERTo/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "tokenizer_config = {\n",
        "\t\"max_len\": 512\n",
        "}\n",
        "with open(\"./EsperBERTo/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri2BIQKqjfHm",
        "colab_type": "text"
      },
      "source": [
        "Let's run our script with the following options:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmaHZXzmkNtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmd =\t\"\"\"\n",
        "  python run_language_modeling.py\n",
        "  \t--train_data_file ./oscar.eo.txt\n",
        "  \t--output_dir ./EsperBERTo-small-v1\n",
        "\t--model_type roberta\n",
        "\t--mlm\n",
        "\t--config_name ./EsperBERTo\n",
        "\t--tokenizer_name ./EsperBERTo\n",
        "\t--do_train\n",
        "\t--line_by_line\n",
        "\t--learning_rate 1e-4\n",
        "\t--num_train_epochs 1\n",
        "\t--save_total_limit 2\n",
        "\t--save_steps 2000\n",
        "\t--per_gpu_train_batch_size 16\n",
        "\t--seed 42\n",
        "\"\"\".replace(\"\\n\", \" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBSrPay8kdB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eaa675e2-6e20-4cf0-8b9b-3e3cc731aeff"
      },
      "source": [
        "%%time\n",
        "!{cmd}"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-02 13:34:40.262434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "05/02/2020 13:34:42 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
            "05/02/2020 13:34:42 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "05/02/2020 13:34:42 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='./EsperBERTo-small-v1', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=0.0001, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir=None, logging_first_step=False, logging_steps=500, save_steps=2000, save_total_limit=2, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1)\n",
            "05/02/2020 13:34:42 - INFO - transformers.configuration_utils -   loading configuration file ./EsperBERTo/config.json\n",
            "05/02/2020 13:34:42 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "05/02/2020 13:34:42 - INFO - transformers.configuration_utils -   loading configuration file ./EsperBERTo/config.json\n",
            "05/02/2020 13:34:42 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "05/02/2020 13:34:42 - INFO - transformers.tokenization_utils -   Model name './EsperBERTo' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming './EsperBERTo' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/02/2020 13:34:42 - INFO - transformers.tokenization_utils -   Didn't find file ./EsperBERTo/added_tokens.json. We won't load it.\n",
            "05/02/2020 13:34:42 - INFO - transformers.tokenization_utils -   Didn't find file ./EsperBERTo/special_tokens_map.json. We won't load it.\n",
            "05/02/2020 13:34:42 - INFO - transformers.tokenization_utils -   loading file ./EsperBERTo/vocab.json\n",
            "05/02/2020 13:34:42 - INFO - transformers.tokenization_utils -   loading file ./EsperBERTo/merges.txt\n",
            "05/02/2020 13:34:42 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/02/2020 13:34:42 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/02/2020 13:34:42 - INFO - transformers.tokenization_utils -   loading file ./EsperBERTo/tokenizer_config.json\n",
            "05/02/2020 13:34:42 - INFO - __main__ -   Training new model from scratch\n",
            "05/02/2020 13:34:47 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ./oscar.eo.txt\n",
            "tcmalloc: large alloc 1250942976 bytes == 0x7ae28000 @  0x7f7890b2c1e7 0x5450df 0x52e319 0x52f3cf 0x53e701 0x4f2b30 0x50a8af 0x50c5b9 0x508245 0x5096b7 0x595311 0x5a522c 0x5a670a 0x4bb19c 0x5bd993 0x50a8af 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509d48 0x50aa7d\n",
            "tcmalloc: large alloc 1223958528 bytes == 0xc5726000 @  0x7f7890b2c1e7 0x5ad4cb 0x4bb356 0x5bd993 0x50a8af 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509d48 0x50aa7d 0x50c5b9 0x508245 0x50b403 0x635222 0x6352d7 0x638a8f 0x639631 0x4b0f40 0x7f7890729b97 0x5b2fda\n",
            "05/02/2020 13:41:53 - INFO - transformers.trainer -   ***** Running training *****\n",
            "05/02/2020 13:41:53 - INFO - transformers.trainer -     Num examples = 974545\n",
            "05/02/2020 13:41:53 - INFO - transformers.trainer -     Num Epochs = 1\n",
            "05/02/2020 13:41:53 - INFO - transformers.trainer -     Instantaneous batch size per GPU = 16\n",
            "05/02/2020 13:41:53 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "05/02/2020 13:41:53 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "05/02/2020 13:41:53 - INFO - transformers.trainer -     Total optimization steps = 60910\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/60910 [00:00<?, ?it/s]\u001b[A/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "\n",
            "Iteration:   0% 1/60910 [00:46<787:07:25, 46.52s/it]\u001b[A\n",
            "Iteration:   0% 2/60910 [01:16<703:14:44, 41.57s/it]\u001b[A\n",
            "Iteration:   0% 3/60910 [02:03<731:29:06, 43.24s/it]\u001b[A\n",
            "Iteration:   0% 4/60910 [02:27<630:47:54, 37.28s/it]\u001b[A\n",
            "Iteration:   0% 5/60910 [02:50<558:03:47, 32.99s/it]\u001b[A\n",
            "Iteration:   0% 6/60910 [03:22<557:26:24, 32.95s/it]\u001b[A\n",
            "Iteration:   0% 7/60910 [03:45<505:14:13, 29.86s/it]\u001b[A\n",
            "Iteration:   0% 8/60910 [04:08<468:24:56, 27.69s/it]\u001b[Atcmalloc: large alloc 1703936000 bytes == 0x2bf8e6000 @  0x7f7890b0eb6b 0x7f7890b2e379 0x7f784296904e 0x7f784296af4a 0x7f787b85967b 0x7f787b44bd3b 0x7f787b44c5d3 0x7f787b208ceb 0x7f787b20a2f7 0x7f787b70ddb8 0x7f787b751ec0 0x7f787d3296fe 0x7f787b751ec0 0x7f787b315b70 0x7f787b311485 0x7f787b31204a 0x7f787b7c6b98 0x7f787d17b184 0x7f787b751ec0 0x7f7889889b43 0x7f78897ec788 0x50ac25 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a067e 0x50d966\n",
            "^C\n",
            "CPU times: user 1.66 s, sys: 281 ms, total: 1.94 s\n",
            "Wall time: 11min 49s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}